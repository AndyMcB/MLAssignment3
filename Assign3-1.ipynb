{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "np.set_printoptions(precision=4)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "################################################\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Header interferes with names - investigate later\n",
    "headerFlag = None\n",
    "delimFlag = ','\n",
    "namesFlag = ['att1', 'att2', 'att3', 'att4', 'OwlLabel']\n",
    "# body-length, wing-length, body-width, wing-width, type.\n",
    "\n",
    "labelColumn = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### CATH\n",
    "\n",
    "def scale_and_encode(df):\n",
    "\n",
    "    # Get Features\n",
    "    # if features are continuous - revisit\n",
    "    feats = df.drop(df.columns[[-1]], axis=1).apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)))\n",
    "\n",
    "    # Get encoded Labels\n",
    "    labels = pd.get_dummies(df[df.columns[-1:]])\n",
    "\n",
    "    # Recombine feats & labels\n",
    "    combine_data = pd.concat([feats, labels], axis=1)\n",
    "    \n",
    "    return combine_data, len(feats.columns)\n",
    "\n",
    "\n",
    "#Gets data in the correct format and adds a bias to the features\n",
    "def feat_label_bias(df, label_ind):\n",
    "\n",
    "    list_data_pre = df.values.tolist()\n",
    "    list_data = [[x[0:label_ind] + [1.0] for x in list_data_pre]] + [[x[label_ind:] for x in list_data_pre]]\n",
    "    \n",
    "    new_df = pd.DataFrame(data=list_data).transpose()\n",
    "    new_df.columns = ['feats', 'labels']\n",
    "\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### ANDREW\n",
    "\n",
    "def load_file(filename, names):\n",
    "    \"\"\" Loads a csv into a dataframe\"\"\"\n",
    "    \n",
    "    print(filename)\n",
    "    data = read_csv(filename, names=names) \n",
    "    return data\n",
    "\n",
    "\n",
    "def train_test_split(data, train_frac=0.666):\n",
    "    \"\"\" Takes a dataset and splits it into a training and test set with measures to ensure even distribution\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    data -- pandas dataframe of data to be split\n",
    "    train_frac -- franction of the data to be used for training, the rest will be used for test\n",
    "    \n",
    "    N.B: The feature labes must be the last column in the dataset\"\"\"\n",
    "    \n",
    "    labels = data[data.columns[-1]] # get last column in dataframe which will contain labels\n",
    "    num_labels = len(labels.unique())\n",
    "    split_threshold = 1/num_labels\n",
    "    \n",
    "    train = data.sample(frac=train_frac, random_state=np.random.RandomState())\n",
    "    test = data.drop(train.index)\n",
    "\n",
    "    unique_labels = labels.unique()\n",
    "  \n",
    "    label_splits_train = [sum(train[train.columns[-1]] == x) for x in unique_labels]\n",
    "    label_splits_test = [sum(test[test.columns[-1]] == x) for x in unique_labels]   \n",
    "    \n",
    "    split_vals_train = [x/len(train) for x in label_splits_train]\n",
    "    split_vals_test = [x/len(test) for x in label_splits_test]\n",
    "    \n",
    "    is_even_split_train = [(split_val < (split_threshold + 0.1)) & (split_val > (split_threshold - 0.1))\n",
    "                           for split_val in split_vals_train] # Ensure a fair split \n",
    "    is_even_split_test = [(split_val < (split_threshold + 0.1)) & (split_val > (split_threshold - 0.1))\n",
    "                           for split_val in split_vals_test]\n",
    "    \n",
    "    if not is_even_split_train and not is_even_split_test:\n",
    "        return train_test_split(data, train_frac)\n",
    "    else:\n",
    "        return train, test\n",
    "    \n",
    "    #split_val: decimal representation of how much one type makes up of the while data set\n",
    "    #split_threshold: how much each of the labels should represent in the data set\n",
    "    \n",
    "    \n",
    "def rand_train_test_split(data, train_frac=0.666):\n",
    "    \"\"\" Takes a dataset and splits it into a training and test set\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    data -- pandas dataframe of data to be split\n",
    "    train_frac -- franction of the data to be used for training, the rest will be used for test\n",
    "    \n",
    "    N.B: The feature labes must be the last column in the dataset\"\"\"\n",
    "    \n",
    "    labels = data[data.columns[-1]] # get last column in dataframe which will contain labels\n",
    "    \n",
    "    sample_size = int(len(data)*train_frac)\n",
    "    \n",
    "    rows = np.random.choice(data.index.values, sample_size)\n",
    "    train = data.ix[rows]\n",
    "    test = data.drop(train.index)\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "\n",
    "def n_random_split(n, data, splitter=rand_train_test_split):\n",
    "    \"\"\" Takes a pandas dataframe and returns n amount of random splits of training and test data in a dict\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    n -- number of training/test sets to return\n",
    "    data -- dataframe to split \n",
    "    splitter -- function which splits the data into train and test\"\"\"\n",
    "    \n",
    "    d = {}\n",
    "    for i in range(n):\n",
    "        train, test = splitter(data)\n",
    "        d[i] = [train, test]\n",
    "        \n",
    "    return d\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:0 \n",
      " Num training samples:59 \n",
      " Num test samples:47\n",
      "\n",
      "Index:1 \n",
      " Num training samples:59 \n",
      " Num test samples:47\n",
      "\n",
      "Index:2 \n",
      " Num training samples:59 \n",
      " Num test samples:43\n",
      "\n",
      "Index:3 \n",
      " Num training samples:59 \n",
      " Num test samples:46\n",
      "\n",
      "Index:4 \n",
      " Num training samples:59 \n",
      " Num test samples:45\n",
      "\n",
      "Index:5 \n",
      " Num training samples:59 \n",
      " Num test samples:43\n",
      "\n",
      "Index:6 \n",
      " Num training samples:59 \n",
      " Num test samples:49\n",
      "\n",
      "Index:7 \n",
      " Num training samples:59 \n",
      " Num test samples:48\n",
      "\n",
      "Index:8 \n",
      " Num training samples:59 \n",
      " Num test samples:47\n",
      "\n",
      "Index:9 \n",
      " Num training samples:59 \n",
      " Num test samples:44\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d = n_random_split(10, data)\n",
    "\n",
    "for index, datasets in d.items():\n",
    "    train_data = datasets[0]\n",
    "    test_data = datasets[1]\n",
    "    info = 'Index:{0} \\n Num training samples:{1} \\n Num test samples:{2}\\n'.format(index, len(train_data), len(test_data))\n",
    "    print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "in_data = pd.read_csv('C:/Users/AMCBR/MyStuff/College Notes & Work/Year 4/Machine Learning & Data Mining/Assignment 3/owls15.csv', header=headerFlag, sep=delimFlag, names=namesFlag, index_col=False)\n",
    "\n",
    "# Make data binary for now\n",
    "in_data = in_data[in_data.OwlLabel != 'BarnOwl']\n",
    "\n",
    "\n",
    "# Scale the features & Encode the labels\n",
    "data_scaled, label_ind = scale_and_encode(in_data)\n",
    "\n",
    "# Get data in the right form & add bias\n",
    "data = feat_label_bias(data_scaled, label_ind)\n",
    "\n",
    "# Split Data into Training and Test Sets\n",
    "train_raw, test_raw = rand_train_test_split(data, train_frac=0.666)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feats</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>[-0.18611111111111178, 0.4761437908496737, 0.4...</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.11388888888888826, -0.2885620915032676, -0....</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[-0.18611111111111178, 0.1232026143790854, 0.2...</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[-0.0861111111111117, -0.2885620915032676, -0....</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[-0.18611111111111178, 0.18202614379085016, 0....</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[-0.18611111111111178, 0.1232026143790854, 0.2...</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>[0.013888888888888395, 0.4173202614379089, 0.4...</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>[0.013888888888888395, 0.3290849673202619, 0.3...</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[-0.18611111111111178, 0.1232026143790854, 0.2...</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.2638888888888884, -0.14150326797385585, -0....</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>[-0.0861111111111117, 0.06437908496732066, 0.2...</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.2638888888888884, -0.14150326797385585, -0....</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[-0.18611111111111178, 0.1232026143790854, 0.2...</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>[0.013888888888888395, 0.3290849673202619, 0.3...</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>[-0.0861111111111117, 0.03496732026143841, 0.2...</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.1638888888888883, -0.2003267973856206, -0.3...</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>[-0.18611111111111178, 0.18202614379085016, 0....</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>[0.21388888888888835, 0.4173202614379089, 0.43...</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>[-0.2361111111111116, 0.005555555555555901, 0....</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>[0.3138888888888882, 0.5643790849673207, 0.541...</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.1638888888888883, -0.2003267973856206, -0.3...</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>[0.06388888888888822, 0.27026143790849716, 0.3...</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[-0.18611111111111178, 0.18202614379085016, 0....</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[-0.0861111111111117, -0.43562091503267936, -0...</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.2638888888888884, -0.14150326797385585, -0....</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.46388888888888813, -0.1709150326797381, -0....</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>[-0.3361111111111117, -0.2591503267973851, 0.1...</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>[0.3138888888888882, 0.5643790849673207, 0.541...</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>[-0.03611111111111165, 0.18202614379085016, 0....</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>[0.3138888888888882, 0.5643790849673207, 0.541...</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>[-0.18611111111111178, 0.18202614379085016, 0....</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.46388888888888813, -0.1709150326797381, -0....</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[-0.03611111111111165, -0.2591503267973851, -0...</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[0.1638888888888883, -0.22973856209150284, -0....</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.11388888888888826, -0.2885620915032676, -0....</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.013888888888888395, -0.31797385620914986, -...</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.2638888888888884, -0.2003267973856206, -0.3...</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.1638888888888883, -0.2003267973856206, -0.3...</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[-0.03611111111111165, -0.2591503267973851, -0...</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.21388888888888835, -0.22973856209150284, -0...</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.013888888888888395, 0.18202614379085016, 0....</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.5138888888888884, -0.0826797385620911, -0.3...</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.11388888888888826, -0.11209150326797335, -0...</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.21388888888888835, -0.22973856209150284, -0...</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>[-0.28611111111111165, 0.09379084967320289, 0....</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[-0.18611111111111178, 0.1232026143790854, 0.2...</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>[-0.0861111111111117, 0.4173202614379089, 0.38...</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.11388888888888826, -0.2885620915032676, -0....</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>[-0.0861111111111117, 0.2114379084967324, 0.38...</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[-0.18611111111111178, 0.18202614379085016, 0....</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>[-0.03611111111111165, 0.3290849673202619, 0.3...</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[0.2638888888888884, -0.14150326797385585, -0....</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>[0.06388888888888822, 0.27026143790849716, 0.3...</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>[-0.18611111111111178, 0.18202614379085016, 0....</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>[-0.2361111111111116, 0.18202614379085016, 0.3...</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.3138888888888882, -0.2003267973856206, -0.2...</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>[0.06388888888888822, 0.27026143790849716, 0.3...</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[0.013888888888888395, 0.18202614379085016, 0....</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>[-0.18611111111111178, 0.15261437908496764, 0....</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                feats      labels\n",
       "83  [-0.18611111111111178, 0.4761437908496737, 0.4...  [0.0, 1.0]\n",
       "7   [0.11388888888888826, -0.2885620915032676, -0....  [1.0, 0.0]\n",
       "47  [-0.18611111111111178, 0.1232026143790854, 0.2...  [0.0, 1.0]\n",
       "18  [-0.0861111111111117, -0.2885620915032676, -0....  [1.0, 0.0]\n",
       "45  [-0.18611111111111178, 0.18202614379085016, 0....  [0.0, 1.0]\n",
       "47  [-0.18611111111111178, 0.1232026143790854, 0.2...  [0.0, 1.0]\n",
       "77  [0.013888888888888395, 0.4173202614379089, 0.4...  [0.0, 1.0]\n",
       "62  [0.013888888888888395, 0.3290849673202619, 0.3...  [0.0, 1.0]\n",
       "47  [-0.18611111111111178, 0.1232026143790854, 0.2...  [0.0, 1.0]\n",
       "35  [0.2638888888888884, -0.14150326797385585, -0....  [1.0, 0.0]\n",
       "69  [-0.0861111111111117, 0.06437908496732066, 0.2...  [0.0, 1.0]\n",
       "35  [0.2638888888888884, -0.14150326797385585, -0....  [1.0, 0.0]\n",
       "47  [-0.18611111111111178, 0.1232026143790854, 0.2...  [0.0, 1.0]\n",
       "62  [0.013888888888888395, 0.3290849673202619, 0.3...  [0.0, 1.0]\n",
       "54  [-0.0861111111111117, 0.03496732026143841, 0.2...  [0.0, 1.0]\n",
       "15  [0.1638888888888883, -0.2003267973856206, -0.3...  [1.0, 0.0]\n",
       "64  [-0.18611111111111178, 0.18202614379085016, 0....  [0.0, 1.0]\n",
       "85  [0.21388888888888835, 0.4173202614379089, 0.43...  [0.0, 1.0]\n",
       "72  [-0.2361111111111116, 0.005555555555555901, 0....  [0.0, 1.0]\n",
       "56  [0.3138888888888882, 0.5643790849673207, 0.541...  [0.0, 1.0]\n",
       "24  [0.1638888888888883, -0.2003267973856206, -0.3...  [1.0, 0.0]\n",
       "61  [0.06388888888888822, 0.27026143790849716, 0.3...  [0.0, 1.0]\n",
       "45  [-0.18611111111111178, 0.18202614379085016, 0....  [0.0, 1.0]\n",
       "12  [-0.0861111111111117, -0.43562091503267936, -0...  [1.0, 0.0]\n",
       "35  [0.2638888888888884, -0.14150326797385585, -0....  [1.0, 0.0]\n",
       "4   [0.46388888888888813, -0.1709150326797381, -0....  [1.0, 0.0]\n",
       "53  [-0.3361111111111117, -0.2591503267973851, 0.1...  [0.0, 1.0]\n",
       "56  [0.3138888888888882, 0.5643790849673207, 0.541...  [0.0, 1.0]\n",
       "55  [-0.03611111111111165, 0.18202614379085016, 0....  [0.0, 1.0]\n",
       "56  [0.3138888888888882, 0.5643790849673207, 0.541...  [0.0, 1.0]\n",
       "64  [-0.18611111111111178, 0.18202614379085016, 0....  [0.0, 1.0]\n",
       "4   [0.46388888888888813, -0.1709150326797381, -0....  [1.0, 0.0]\n",
       "9   [-0.03611111111111165, -0.2591503267973851, -0...  [1.0, 0.0]\n",
       "30  [0.1638888888888883, -0.22973856209150284, -0....  [1.0, 0.0]\n",
       "25  [0.11388888888888826, -0.2885620915032676, -0....  [1.0, 0.0]\n",
       "1   [0.013888888888888395, -0.31797385620914986, -...  [1.0, 0.0]\n",
       "8   [0.2638888888888884, -0.2003267973856206, -0.3...  [1.0, 0.0]\n",
       "15  [0.1638888888888883, -0.2003267973856206, -0.3...  [1.0, 0.0]\n",
       "42  [-0.03611111111111165, -0.2591503267973851, -0...  [1.0, 0.0]\n",
       "3   [0.21388888888888835, -0.22973856209150284, -0...  [1.0, 0.0]\n",
       "46  [0.013888888888888395, 0.18202614379085016, 0....  [0.0, 1.0]\n",
       "19  [0.5138888888888884, -0.0826797385620911, -0.3...  [1.0, 0.0]\n",
       "26  [0.11388888888888826, -0.11209150326797335, -0...  [1.0, 0.0]\n",
       "3   [0.21388888888888835, -0.22973856209150284, -0...  [1.0, 0.0]\n",
       "86  [-0.28611111111111165, 0.09379084967320289, 0....  [0.0, 1.0]\n",
       "47  [-0.18611111111111178, 0.1232026143790854, 0.2...  [0.0, 1.0]\n",
       "68  [-0.0861111111111117, 0.4173202614379089, 0.38...  [0.0, 1.0]\n",
       "25  [0.11388888888888826, -0.2885620915032676, -0....  [1.0, 0.0]\n",
       "88  [-0.0861111111111117, 0.2114379084967324, 0.38...  [0.0, 1.0]\n",
       "45  [-0.18611111111111178, 0.18202614379085016, 0....  [0.0, 1.0]\n",
       "80  [-0.03611111111111165, 0.3290849673202619, 0.3...  [0.0, 1.0]\n",
       "35  [0.2638888888888884, -0.14150326797385585, -0....  [1.0, 0.0]\n",
       "76  [0.06388888888888822, 0.27026143790849716, 0.3...  [0.0, 1.0]\n",
       "64  [-0.18611111111111178, 0.18202614379085016, 0....  [0.0, 1.0]\n",
       "89  [-0.2361111111111116, 0.18202614379085016, 0.3...  [0.0, 1.0]\n",
       "17  [0.3138888888888882, -0.2003267973856206, -0.2...  [1.0, 0.0]\n",
       "76  [0.06388888888888822, 0.27026143790849716, 0.3...  [0.0, 1.0]\n",
       "46  [0.013888888888888395, 0.18202614379085016, 0....  [0.0, 1.0]\n",
       "75  [-0.18611111111111178, 0.15261437908496764, 0....  [0.0, 1.0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import choice \n",
    "from numpy import array, dot,random \n",
    "\n",
    "def train_perceptron(train_dat):\n",
    "\n",
    "    unit_step = lambda x: 0 if x < 0 else 1 \n",
    "\n",
    "    w = random.rand(3) \n",
    "    errors = [] \n",
    "    learning_rate = 0.2 \n",
    "    iter_no = 0\n",
    "    iterations = 5\n",
    "    \n",
    "    for row in train_dat.iterrows():\n",
    "        if iter_no < iterations:\n",
    "            iter_no = iter_no + 1\n",
    "            print(str(row[0]) + \" is the row index\")\n",
    "            print('Feats: ', row[1][0])\n",
    "            print('Labels: ', row[1][1])\n",
    "            print()\n",
    "    \n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_perceptron(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C = []\n",
    "A = time.time()\n",
    "for ir in t.itertuples():\n",
    "    C.append((ir[1], ir[2]))    \n",
    "B.append(time.time()-A)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import choice \n",
    "from numpy import array, dot,random \n",
    "\n",
    "unit_step = lambda x: 0 if x < 0 else 1 \n",
    "\n",
    "# Extra 1s are biases\n",
    "training_data = [ \n",
    "    (array([0,0,1]), 0), \n",
    "    (array([0,1,1]), 1), \n",
    "    (array([1,0,1]), 1), \n",
    "    (array([1,1,1]), 1), \n",
    "    ]\n",
    "    \n",
    "w = random.rand(3) \n",
    "errors = [] \n",
    "learning_rate = 0.2 \n",
    "iterations = 100 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('weights = ', w)\n",
    "print()\n",
    "\n",
    "#  for i in range(iterations): \n",
    "x, expected = choice(training_data) # randomly samples a data point from training_data - gives feats, lab\n",
    "result = dot(w, x) # weights by inputs\n",
    "error = expected - unit_step(result) #lab 1/0 - 0 if result>0, else 1\n",
    "errors.append(error) \n",
    "w += learning_rate * error * x \n",
    "\n",
    "print('Train input = ', x, ',', expected)\n",
    "print()\n",
    "\n",
    "print('weights x feats = ', result)\n",
    "print()\n",
    "print('expected - unit_step(result) #lab 1/0 - 0 if result>0, else 1 = ', error)\n",
    "print()\n",
    "print('errors = ', errors)\n",
    "print()\n",
    "print('new weights = ', w)\n",
    "print()\n",
    "\n",
    "# print(errors)\n",
    "\n",
    "# for x, _ in training_data: \n",
    "#     result = dot(x, w) \n",
    "#     print(\"{}: {} -> {}\".format(x[:3], result, unit_step(result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Andrew Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_split(data, train_frac=0.666):\n",
    "    \n",
    "    num_labels = len(data.OwlLabel.unique())\n",
    "    split_threshold = 1/num_labels\n",
    "    \n",
    "    train = data.sample(frac=train_frac, random_state=np.random.RandomState())\n",
    "    test = data.drop(train.index)\n",
    "\n",
    "    split_val = len(train.loc[train['OwlLabel'] == 'LongEaredOwl'])/len(train) #De-hardcode type values\n",
    "    is_even_split_train =  (split_val < (split_threshold + 0.1)) & (split_val > (split_threshold - 0.1)) # Ensure a fair split of all types of owl\n",
    "  \n",
    "#     print('split val: {}'.format(split_val))\n",
    "#     print('split threshold: {}'.format(split_threshold))\n",
    "    \n",
    "#     print(len(train))\n",
    "#     print(train)\n",
    "#     print(len(test))\n",
    "#     print(test)\n",
    "    \n",
    "    if not is_even_split_train:\n",
    "        return train_test_split(data, train_frac)\n",
    "    else:\n",
    "        return train, test\n",
    "    \n",
    "    #split_val: decimal representation of how much one type makes up of the while data set\n",
    "    #split_threshold: how much each of the labels should represent in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
