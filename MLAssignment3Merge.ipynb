{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/AMCBR/MyStuff/College Notes & Work/Year 4/Machine Learning & Data Mining/Assignment 3/owls15.csv\n"
     ]
    }
   ],
   "source": [
    "# Andrew\n",
    "\n",
    "filename = input()\n",
    "\n",
    "#C:/Users/AMCBR/MyStuff/College Notes & Work/Year 4/Machine Learning & Data Mining/Assignment 3/owls15.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Catherine\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array, dot, random\n",
    "import random\n",
    "from pandas import read_csv\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "np.set_printoptions(precision=4)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catherine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Catherine\n",
    "### Data Prep\n",
    "\n",
    "def scale_and_encode(df):\n",
    "\n",
    "    # Get Features\n",
    "    # if features are continuous - revisit\n",
    "    feats = df.drop(df.columns[[-1]], axis=1).apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)))\n",
    "\n",
    "    # Get encoded Labels\n",
    "    labels = pd.get_dummies(df[df.columns[-1:]])\n",
    "    \n",
    "\n",
    "    # Recombine feats & labels\n",
    "    combine_data = pd.concat([feats, labels], axis=1)\n",
    "    \n",
    "    return combine_data, len(feats.columns), list(labels.columns.values)\n",
    "\n",
    "\n",
    "# Gets data in the correct format and adds a bias to the features\n",
    "def feat_label_bias(df, label_ind):\n",
    "\n",
    "    list_data_pre = df.values.tolist()\n",
    "    list_data = [[x[:label_ind] + [1.0] for x in list_data_pre]] + [[x[label_ind:] for x in list_data_pre]]\n",
    "    \n",
    "    new_df = pd.DataFrame(data=list_data).transpose()\n",
    "    new_df.columns = ['feats', 'labels']\n",
    "\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Catherine\n",
    "# Perceptron Funcs\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+math.e**-x)\n",
    "\n",
    "def update_weights(w, learning_rate, error, feature_vec):\n",
    "    return w + [(learning_rate * error) * f for f in feature_vec] \n",
    "\n",
    "def train_perceptron(train_dat):\n",
    "\n",
    "    \n",
    "    num_feats = len(train_dat.head(1).values[0][0]) \n",
    "    num_classes = len(train_dat.head(1).values[0][1]) \n",
    "\n",
    "    \n",
    "    w = np.random.rand(num_classes, num_feats) \n",
    "\n",
    "\n",
    "  \n",
    "    errors = [] \n",
    "    learning_rate = 0.12\n",
    "    curr_iter = 0\n",
    "    iterations = 300\n",
    "    num_epochs = 2\n",
    "    \n",
    "    for epoch in range(0, num_epochs):\n",
    "    \n",
    "        for row in train_dat.iterrows():\n",
    "\n",
    "            # Use specified number of training data points\n",
    "            if curr_iter < iterations:\n",
    "                curr_iter = curr_iter + 1\n",
    "\n",
    "                # Feats and labels\n",
    "                tp_feats = row[1][0]\n",
    "                tp_labels = row[1][1]\n",
    "                class_pred = [0.0] * len(tp_labels)\n",
    "\n",
    "                # Calculate class prediction list \n",
    "                preds_list = []\n",
    "\n",
    "                for i in range(0, num_classes):\n",
    "\n",
    "                    pred = sigmoid(dot(w[i], tp_feats)) # weights by features\n",
    "                    preds_list.append(pred)\n",
    "\n",
    "\n",
    "                class_pred[np.argmax(preds_list)] = 1.0\n",
    "\n",
    "                for i in range(0, num_classes):\n",
    "\n",
    "                    error = tp_labels[i] - preds_list[i]\n",
    "                    errors.append(error)\n",
    "\n",
    "                    w[i] = update_weights(w[i], learning_rate, error, tp_feats)\n",
    "\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Catherine\n",
    "# Predictions & Testing perceptron\n",
    "\n",
    "def predict_one_case(w, feature_list):\n",
    "\n",
    "    num_classes, num_feats = w.shape\n",
    "    \n",
    "    \n",
    "    # Calculate class prediction list \n",
    "    preds_list = []\n",
    "    class_pred = [0.0] * num_classes\n",
    "\n",
    "    for i in range(0, num_classes):\n",
    "\n",
    "        pred = sigmoid(dot(w[i], feature_list)) # weights by features\n",
    "        preds_list.append(pred)\n",
    "\n",
    "    class_pred[np.argmax(preds_list)] = 1.0\n",
    "\n",
    "    return class_pred\n",
    "\n",
    "\n",
    "\n",
    "def predict(w, pred_data):\n",
    "    \n",
    "    new_df_list = []\n",
    "    \n",
    "    for row in pred_data.iterrows():\n",
    "        feats = row[1][0]\n",
    "        \n",
    "        curr_pred = predict_one_case(w, feats)\n",
    "        \n",
    "        new_df_row = [feats, curr_pred]\n",
    "        new_df_list.append(new_df_row)\n",
    "    \n",
    "    new_df = pd.DataFrame(data=new_df_list)\n",
    "    new_df.columns = ['feats', 'prediction']\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "\n",
    "def test_perceptron_accuracy(w, test_data):\n",
    "    \n",
    "    new_df_list = []\n",
    "    \n",
    "    for row in test_data.iterrows():\n",
    "        feats = row[1][0]\n",
    "        labels = row[1][1]\n",
    "        \n",
    "        curr_pred = predict_one_case(w, feats)\n",
    "        \n",
    "        new_df_row = [labels, curr_pred]\n",
    "        new_df_list.append(new_df_row)\n",
    "    \n",
    "    new_df = pd.DataFrame(data=new_df_list)\n",
    "    new_df.columns = ['labels', 'prediction']\n",
    "    \n",
    "    num_true = 0\n",
    "    num_false = 0\n",
    "    \n",
    "    for row in new_df.iterrows():\n",
    "        labs = row[1][0]\n",
    "        preds = row[1][1]\n",
    "        \n",
    "        if labs == preds:\n",
    "            num_true = num_true + 1\n",
    "        else:\n",
    "            num_false = num_false + 1\n",
    "    \n",
    "    accuracy = num_true/(num_true+num_false)\n",
    "#     print('T: ', num_true)\n",
    "#     print('F: ', num_false)\n",
    "    \n",
    "    return accuracy, new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Andrew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_file(filename, names):\n",
    "    \"\"\" Loads a csv into a dataframe\"\"\"\n",
    "    \n",
    "    print(filename)\n",
    "    data = read_csv(filename, names=names) \n",
    "    return data\n",
    "\n",
    "\n",
    "def train_test_split(data, train_frac=0.666):\n",
    "    \"\"\" Takes a dataset and splits it into a training and test set with measures to ensure even distribution\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    data -- pandas dataframe of data to be split\n",
    "    train_frac -- franction of the data to be used for training, the rest will be used for test\n",
    "    \n",
    "    N.B: The feature labes must be the last column in the dataset\"\"\"\n",
    "    \n",
    "    labels = data[data.columns[-1]] # get last column in dataframe which will contain labels\n",
    "    num_labels = len(labels.unique())\n",
    "    split_threshold = 1/num_labels\n",
    "    \n",
    "    train = data.sample(frac=train_frac, random_state=np.random.RandomState())\n",
    "    test = data.drop(train.index)\n",
    "\n",
    "    unique_labels = labels.unique()\n",
    "  \n",
    "    label_splits_train = [sum(train[train.columns[-1]] == x) for x in unique_labels]\n",
    "    label_splits_test = [sum(test[test.columns[-1]] == x) for x in unique_labels]   \n",
    "    \n",
    "    split_vals_train = [x/len(train) for x in label_splits_train]\n",
    "    split_vals_test = [x/len(test) for x in label_splits_test]\n",
    "    \n",
    "    is_even_split_train = [(split_val < (split_threshold + 0.1)) & (split_val > (split_threshold - 0.1))\n",
    "                           for split_val in split_vals_train] # Ensure a fair split \n",
    "    is_even_split_test = [(split_val < (split_threshold + 0.1)) & (split_val > (split_threshold - 0.1))\n",
    "                           for split_val in split_vals_test]\n",
    "    \n",
    "    if not is_even_split_train and not is_even_split_test:\n",
    "        return train_test_split(data, train_frac)\n",
    "    else:\n",
    "        return train, test\n",
    "    \n",
    "    #split_val: decimal representation of how much one type makes up of the while data set\n",
    "    #split_threshold: how much each of the labels should represent in the data set\n",
    "    \n",
    "    \n",
    "def rand_train_test_split(data, train_frac=0.666):\n",
    "    \"\"\" Takes a dataset and splits it into a training and test set\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    data -- pandas dataframe of data to be split\n",
    "    train_frac -- franction of the data to be used for training, the rest will be used for test\n",
    "    \n",
    "    N.B: The feature labes must be the last column in the dataset\"\"\"\n",
    "    \n",
    "    labels = data[data.columns[-1]] # get last column in dataframe which will contain labels\n",
    "    \n",
    "    sample_size = int(len(data)*train_frac)\n",
    "    \n",
    "    train = data.sample(frac=train_frac, random_state=np.random.RandomState())\n",
    "    test = data.drop(train.index)\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "\n",
    "def n_random_split(n, data, splitter=rand_train_test_split):\n",
    "    \"\"\" Takes a pandas dataframe and returns n amount of random splits of training and test data in a dict\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    n -- number of training/test sets to return\n",
    "    data -- dataframe to split \n",
    "    splitter -- function which splits the data into train and test\"\"\"\n",
    "    \n",
    "    d = {}\n",
    "    for i in range(n):\n",
    "        train, test = splitter(data)\n",
    "        d[i] = [train, test]\n",
    "        \n",
    "    return d\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN - single train/test cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.7333333333333333\n"
     ]
    }
   ],
   "source": [
    "#Andrew\n",
    "\n",
    "# Load Data\n",
    "in_data = pd.read_csv(filename, header=None, sep=',', index_col=False)\n",
    "\n",
    "# Scale the features & Encode the labels\n",
    "data_scaled, label_ind, data_labels = scale_and_encode(in_data)\n",
    "\n",
    "# Get data in the right form & add bias\n",
    "data_with_bias = feat_label_bias(data_scaled, label_ind)\n",
    "\n",
    "# Split Data into Training and Test Sets\n",
    "train_data, test_data = rand_train_test_split(data_with_bias, train_frac=0.666)\n",
    "\n",
    "# Train perceptron\n",
    "perceptron_weights = train_perceptron(train_data)\n",
    "\n",
    "# Check the accuracy of the perceptron\n",
    "accuracy, prediction_data = test_perceptron_accuracy(perceptron_weights, test_data)\n",
    "\n",
    "print('Accuracy = ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Avg. Over 10 Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6222222222222222, 0.8, 0.8666666666666667, 0.8222222222222222, 0.7333333333333333, 0.6888888888888889, 0.8444444444444444, 0.6888888888888889, 0.8888888888888888, 0.9111111111111111]\n",
      "\n",
      "Average Accuracy over 10 iterations: 0.7866666666666667\n"
     ]
    }
   ],
   "source": [
    "#Andrew\n",
    "\n",
    "d = n_random_split(10, data_with_bias)\n",
    "\n",
    "train_sets=[]\n",
    "test_sets=[]\n",
    "for index, datasets in d.items():\n",
    "    train_sets.append(datasets[0])\n",
    "    test_sets.append(datasets[1])\n",
    "\n",
    "accuracy_scores=[] \n",
    "scores_by_iteration = {}\n",
    "    \n",
    "for i in range(len(train_sets)):\n",
    "    perceptron_weights = train_perceptron(train_sets[i])\n",
    "    accuracy, prediction_data = test_perceptron_accuracy(perceptron_weights, test_sets[i])\n",
    "    accuracy_scores.append(accuracy)\n",
    "    scores_by_iteration[i] = [accuracy, prediction_data]\n",
    "    \n",
    "print(accuracy_scores)\n",
    "avg = sum(accuracy_scores)/len(accuracy_scores)\n",
    "print(\"\\nAverage Accuracy over 10 iterations: {}\".format(avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Output to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Andrew\n",
    "\n",
    "file = open('output.txt', 'w')\n",
    "\n",
    "\n",
    "file.write('The 3 columns correspond to:\\n {}\\n\\n'.format(str(data_labels)))\n",
    "\n",
    "for key, val in scores_by_iteration.items():\n",
    "    file.write(\"Iteration {}:\\n\\n\".format(key))\n",
    "    file.write(str(prediction_data))\n",
    "    file.write('\\nAccuracy: {}'.format(val[0]))\n",
    "    file.write(\"\\n\\n\")\n",
    "\n",
    "\n",
    "file.write(\"\\nAverage Accuracy over 10 iterations: {}\".format(avg))    #if data is a pandas.DataFrame object\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: train_test_split\n",
      "Passed: True\n",
      "\n",
      "Test: rand_train_test_split\n",
      "Passed: True\n"
     ]
    }
   ],
   "source": [
    "# Andrew\n",
    "\n",
    "passed = False\n",
    "\n",
    "####Test: train_test_split\n",
    "print('Test: train_test_split')\n",
    "frac=0.666\n",
    "train, test = train_test_split(data1, train_frac=frac)\n",
    "correct_size = (len(train)+len(test) == size)\n",
    "correct_split = (frac+0.1 > len(train)/size > frac-0.1 ) \n",
    "\n",
    "passed = correct_size and correct_split\n",
    "print('Passed: {}'.format(passed))\n",
    "\n",
    "\n",
    "\n",
    "####Test: rand_train_test_split\n",
    "print('\\nTest: rand_train_test_split')\n",
    "frac=0.666\n",
    "train, test = rand_train_test_split(data1, train_frac=frac)\n",
    "correct_size = (len(train)+len(test) == size)\n",
    "correct_split = (frac+0.1 > len(train)/size > frac-0.1 ) \n",
    "\n",
    "passed = correct_size and correct_split\n",
    "print('Passed: {}'.format(passed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
